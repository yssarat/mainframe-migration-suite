AWSTemplateFormatVersion: '2010-09-09'
Description: 'Asynchronous Mainframe Documentation Analyzer'

Parameters:
  Environment:
    Type: String
    Default: dev
    AllowedValues:
      - dev
      - staging
      - prod
    Description: Environment name

  BedrockModelId:
    Type: String
    Default: 'arn:aws:bedrock:us-east-1:663362705389:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0'
    Description: Bedrock model ARN to use for analysis
  
  MaxFilesPerJob:
    Type: Number
    Default: 100
    Description: Maximum number of files to process per job
  
  ChunkingThreshold:
    Type: Number
    Default: 100000
    Description: Maximum combined characters for analysis

  MaxCombinedChars:
    Type: Number
    Default: 100000
    Description: Maximum combined characters for analysis

  MaxTokensPerChunk:
    Type: Number
    Default: 15000
    Description: Maximum tokens per chunk

  ParameterStorePrefix:
    Type: String
    Default: /mainframe-modernization/documentation-agent/dev/updated-prompt
    Description: Parameter Store parameter for LLM prompt

  LambdaCodeBucket:
    Type: String
    Description: S3 bucket containing Lambda code packages
    Default: ''
  
  LambdaTimeout:
    Type: Number
    Default: 600
    Description: Timeout for Lambda functions in seconds
  
  LambdaMemory:
    Type: Number
    Default: 2048
    Description: Memory allocation for Lambda functions in MB

Conditions:
  CreateLambdaBucket: !Equals [!Ref LambdaCodeBucket, '']

Resources:
  # Parameter Store for Prompt Template
  PromptTemplate:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub /mainframe-modernization/documentation-agent/${Environment}/updated-prompt
      Type: String
      Tier: Advanced
      Value: |
        As an AWS developer analyzing mainframe documentation located in S3 bucket '{bucket}' under folder '{folder}', your task is to:

        1. CONTEXT:
        - Analyze mainframe program documentation thoroughly and systematically
        - Identify core business logic, data flows, and processing patterns
        - Map mainframe concepts to the most appropriate AWS services based on functionality, not limited to those mentioned in this prompt
        - Consider scalability, reliability, security, and AWS best practices
        - Maintain consistency in your analysis approach across all mainframe artifacts
        - Preserve all data structures and record types exactly as they appear in mainframe components
        - Ensure file extensions match the appropriate AWS service output format
        
        2. REQUIREMENTS ANALYSIS:
        - Extract key business rules and workflows with consistent methodology
        - Document all data structures and relationships in standardized format
        - Map dependencies between modules using consistent notation
        - Catalog batch processes and scheduling requirements systematically
        - Document I/O operations and file handling patterns comprehensively
        - Use appropriate file extensions for all output files (.json, .yaml, .py, etc.)
        - Create detailed AWS architecture diagrams with consistent notation
        
        3. AWS SERVICE SELECTION:
        - Consider the FULL range of AWS services for each mainframe component
        - Evaluate each service based on:
            * Functional requirements match
            * Performance characteristics
            * Cost efficiency
            * Operational complexity
            * Integration capabilities
            * Scalability and elasticity
        - Document your reasoning for EACH service selection in a dedicated reasoning.md file
        - For similar mainframe components, use consistent service mapping patterns
        - Do not create redundant execution paths for the same functionality
        
        4. AWS SERVICE MAPPING GUIDELINES:
        - For batch processing: Consider AWS Batch, AWS Glue, Step Functions, or Lambda
        - For data storage: Evaluate across RDS, DynamoDB, S3, ElastiCache, Neptune, DocumentDB, etc. based on:
            * Data structure (relational, document, graph, key-value)
            * Access patterns and query requirements
            * Transaction and consistency requirements
            * Volume and velocity of data
        - For inter-service communication:
            * Synchronous: API Gateway, AppSync, Application Load Balancer
            * Asynchronous: SQS, SNS, EventBridge, Kinesis, MSK
        - For file processing: S3 events, Lambda, AWS Transfer Family
        - For security: IAM, KMS, Secrets Manager, WAF, Shield
        - For monitoring: CloudWatch, X-Ray, CloudTrail
        
        5. IMPLEMENTATION DESIGN:
        - Create modular components aligned with single responsibility principle
        - Implement comprehensive error handling and logging
        - Include input validation and data integrity checks
        - Set up appropriate IAM roles with least privilege permissions
        - Configure environment variables for configuration management
        - Implement retry mechanisms and circuit breakers where appropriate
        - Use the latest stable versions of all runtimes and services
        - Design for high availability and disaster recovery
        
        6. INTEGRATION PATTERNS:
        - Define clear API contracts for all service interfaces
        - Document message formats and schemas for event-driven communication
        - Specify error handling, retry logic, and recovery procedures
        - Include comprehensive monitoring, alerting, and observability requirements
        - Design consistent state management across distributed components
        
        Based on the above analysis, please generate your response in a structured format with clearly separated sections for each AWS service type. Generate appropriate file extensions for all output artifacts. Use the following format with clear section markers:
        
        ## LAMBDA_FUNCTIONS
        [All Lambda function code, configurations, and explanations]
        
        ## IAM_ROLES
        [All IAM roles, policies, and permissions]
        
        ## DYNAMODB
        [DynamoDB table designs, access patterns, and configurations]
        
        ## RDS
        [RDS table designs, access patterns, and configurations]
         
        ## API_GATEWAY
        [API Gateway configurations, routes, and integrations]
        
        ## AWS_GLUE
        [Glue jobs, scripts, configurations and integrations]
        
        ## S3
        [S3 bucket configurations, lifecycle policies, and access patterns]
        
        ## SQS_SNS_EVENTBRIDGE
        [SQS/SNS/EventBridge configurations, topics, queues, and subscriptions]
        
        ## STEP_FUNCTIONS
        [Step Functions state machines and workflow definitions]
        
        ## OTHER_SERVICES
        [Any other AWS services not covered above, with clear subsections]
        
        7. MANDATORY OUTPUT FILES:
        You must generate two separate files with the following exact names and contents:
        
        a) readme.md
        Must contain:
        - Executive summary of the modernization approach
        - Architecture diagram in ASCII format
        - Implementation roadmap and dependencies
        - Data flow descriptions
        - Security considerations
        - Cost optimization strategies
        - Monitoring and logging approach
        - Error handling strategies
        - Deployment instructions
        
        b) reasoning.md
        Must contain:
        - Detailed service selection reasoning for EACH mainframe component
        - Explicit justification for why each AWS service was chosen
        - Alternative services considered and why they were not selected
        - Trade-offs and compromises made in the design
        - Consistency explanations for similar components
        - Performance considerations
        - Scalability analysis
        - Operational complexity assessment
        
        8. CONSISTENCY REQUIREMENTS:
        - Use identical service mappings for identical mainframe components
        - Apply consistent naming conventions across all resources
        - Maintain uniform documentation structure for all components
        - Use standardized patterns for error handling and logging
        - Apply consistent security controls across all services
        - Document all assumptions made during analysis
        - Provide the same level of detail for all components
        
        Please ensure each section is comprehensive and includes all relevant details for that service type. Include proper error handling, logging, monitoring, and follow AWS Well-Architected Framework principles throughout. Aim for maximum consistency in your analysis and recommendations across similar mainframe artifacts.
  
  # DynamoDB Table for Job Tracking
  JobsTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub MainframeAnalyzerJobs-${Environment}
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: job_id
          AttributeType: S
      KeySchema:
        - AttributeName: job_id
          KeyType: HASH
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

  # IAM Role for Lambda Functions
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: MainframeAnalyzerLambdaPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource: '*'
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource: !GetAtt JobsTable.Arn
              - Effect: Allow
                Action:
                  - bedrock:InvokeModel
                Resource: '*'
              - Effect: Allow
                Action:
                  - states:StartExecution
                  - states:DescribeExecution
                Resource: '*'
              - Effect: Allow
                Action:
                  - ssm:GetParameter
                Resource: !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/mainframe-modernization/documentation-agent/*'

  # IAM Role for Step Functions
  StepFunctionsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: MainframeAnalyzerStepFunctionsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: '*'

  # Lambda Functions
  InitialLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub MainframeAnalyzer-Initial-${Environment}
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/initial-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          STATE_MACHINE_ARN: !Ref MainframeAnalyzerStateMachine
          JOBS_TABLE_NAME: !Ref JobsTable
          MAX_FILES: !Ref MaxFilesPerJob

  PyPDFDocxLayer:
    Type: 'AWS::Lambda::LayerVersion'
    Properties:
      LayerName: pypdfdocx-layer
      Description: Layer containing PyPDF and DocX packages
      Content:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: layer/pypdfdocxlayer.zip
      CompatibleRuntimes:
        - python3.13
        - python3.12
        - python3.11

  ProcessFileLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ProcessFile
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/process-file-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Layers:
        - !Ref PyPDFDocxLayer
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable

  AggregateLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Aggregate
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/aggregate-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          MAX_COMBINED_CHARS: !Ref MaxCombinedChars
          PARAMETER_STORE_PREFIX: !Ref ParameterStorePrefix

  ChunkingLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Chunking
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/chunking-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          CHUNKING_THRESHOLD: !Ref ChunkingThreshold
          MAX_TOKENS_PER_CHUNK: !Ref MaxTokensPerChunk

  ChunkProcessorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ChunkProcessor
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/chunk-processor-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          BEDROCK_MODEL_ID: !Ref BedrockModelId
          PROMPT_TEMPLATE_PATH: !Sub /mainframe-modernization/documentation-agent/${Environment}/updated-prompt

  ResultAggregatorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ResultAggregator
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/result-aggregator-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          PROMPT_TEMPLATE_PATH: !Sub /mainframe-modernization/documentation-agent/${Environment}/updated-prompt

  AnalysisLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Analysis
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/analysis-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          BEDROCK_MODEL_ID: !Ref BedrockModelId

  StatusLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Status
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/status-lambda.zip
      Runtime: python3.13
      Timeout: 30
      MemorySize: 256
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable

  # Step Functions State Machine
  MainframeAnalyzerStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub MainframeAnalyzerWorkflow-${Environment}
      RoleArn: !GetAtt StepFunctionsExecutionRole.Arn
      DefinitionString: !Sub |
        {
          "Comment": "Document Analysis Workflow with Chunking (No Initial Processing)",
          "StartAt": "ProcessFiles",
          "States": {
            "ProcessFiles": {
              "Type": "Map",
              "ItemsPath": "$.files",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "file_key.$": "$$.Map.Item.Value.key",
                "output_path.$": "$.output_path"
              },
              "Iterator": {
                "StartAt": "ProcessFile",
                "States": {
                  "ProcessFile": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "Parameters": {
                      "FunctionName": "${ProcessFileLambda.Arn}",
                      "Payload.$": "$"
                    },
                    "OutputPath": "$.Payload",
                    "End": true
                  }
                }
              },
              "ResultPath": "$.results",
              "Next": "AggregateFiles"
            },
            "AggregateFiles": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${AggregateLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "file_results.$": "$.results"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "EvaluateInputSize"
            },
            "EvaluateInputSize": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${ChunkingLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "full_prompt_key.$": "$.full_prompt_key",
                  "output_path.$": "$.output_path"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "NeedsChunking"
            },
            "NeedsChunking": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.requires_chunking",
                  "BooleanEquals": true,
                  "Next": "ProcessChunks"
                }
              ],
              "Default": "PrepareStandardAnalysis"
            },
            "PrepareStandardAnalysis": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count": 0,
                "max_retries": 5
              },
              "Next": "StandardAnalysis"
            },
            "StandardAnalysis": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${AnalysisLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "full_prompt_key.$": "$.full_prompt_key",
                  "output_path.$": "$.output_path"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": ["States.TaskFailed"],
                  "IntervalSeconds": 180,
                  "MaxAttempts": 3,
                  "BackoffRate": 1.5
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": ["States.ALL"],
                  "ResultPath": "$.lambda_error",
                  "Next": "CheckStandardAnalysisError"
                }
              ],
              "OutputPath": "$.Payload",
              "Next": "CheckStandardAnalysisResult"
            },
            "CheckStandardAnalysisResult": {
              "Type": "Choice",
              "Choices": [
                {
                  "And": [
                    {
                      "Variable": "$.status",
                      "StringEquals": "error"
                    },
                    {
                      "Variable": "$.error",
                      "StringMatches": "*ThrottlingException*"
                    },
                    {
                      "Variable": "$.retry_count",
                      "NumericLessThan": 5
                    }
                  ],
                  "Next": "IncrementStandardRetryAndWait"
                },
                {
                  "Variable": "$.status",
                  "StringEquals": "error",
                  "Next": "UpdateStatusWithError"
                }
              ],
              "Default": "UpdateStatus"
            },
            "CheckStandardAnalysisError": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.lambda_error.errorMessage",
                  "StringMatches": "*Task timed out*",
                  "Next": "HandleStandardTimeout"
                }
              ],
              "Default": "UpdateStatusWithError"
            },
            "HandleStandardTimeout": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count.$": "$.retry_count",
                "max_retries.$": "$.max_retries",
                "error": "Lambda function timed out after 600 seconds"
              },
              "Next": "CheckStandardRetryCount"
            },
            "CheckStandardRetryCount": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.retry_count",
                  "NumericLessThan": 5,
                  "Next": "IncrementStandardRetryAndWait"
                }
              ],
              "Default": "UpdateStatusWithError"
            },
            "IncrementStandardRetryAndWait": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count.$": "States.MathAdd($.retry_count, 1)",
                "max_retries.$": "$.max_retries",
                "wait_seconds": 180
              },
              "Next": "WaitBeforeStandardRetry"
            },
            "WaitBeforeStandardRetry": {
              "Type": "Wait",
              "Seconds": 180,
              "Next": "StandardAnalysis"
            },
            "ProcessChunks": {
              "Type": "Map",
              "ItemsPath": "$.chunks",
              "ItemSelector": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "chunk_key.$": "$$.Map.Item.Value.chunk_key",
                "chunk_index.$": "$$.Map.Item.Value.index",
                "total_chunks.$": "$$.Map.Item.Value.total",
                "output_path.$": "$.output_path",
                "wait_seconds": 120,
                "retry_count": 0,
                "max_retries": 5
              },
              "Iterator": {
                "StartAt": "ProcessChunk",
                "States": {
                  "ProcessChunk": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "Parameters": {
                      "FunctionName": "${ChunkProcessorLambda.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": ["States.TaskFailed"],
                        "IntervalSeconds": 180,
                        "MaxAttempts": 3,
                        "BackoffRate": 1.5
                      }
                    ],
                    "Catch": [
                      {
                        "ErrorEquals": ["States.ALL"],
                        "ResultPath": "$.lambda_error",
                        "Next": "CheckLambdaError"
                      }
                    ],
                    "OutputPath": "$.Payload",
                    "Next": "CheckForThrottling"
                  },
                  "CheckLambdaError": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "Variable": "$.lambda_error.errorMessage",
                        "StringMatches": "*Task timed out*",
                        "Next": "HandleLambdaTimeout"
                      }
                    ],
                    "Default": "PrepareErrorFields"
                  },
                  "PrepareErrorFields": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "chunk_index.$": "$.chunk_index",
                      "error.$": "States.JsonToString($.lambda_error)",
                      "status": "error",
                      "message": "Chunk processing failed after retries"
                    },
                    "Next": "HandleChunkError"
                  },
                  "HandleLambdaTimeout": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "$.retry_count",
                      "max_retries.$": "$.max_retries",
                      "error": "Lambda function timed out after 600 seconds"
                    },
                    "Next": "CheckRetryCount"
                  },
                  "CheckRetryCount": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "Variable": "$.retry_count",
                        "NumericLessThan": 5,
                        "Next": "IncrementRetryAndWait"
                      }
                    ],
                    "Default": "PrepareErrorFields"
                  },
                  "CheckForThrottling": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.status",
                            "StringEquals": "error"
                          },
                          {
                            "Variable": "$.error",
                            "StringMatches": "*ThrottlingException*"
                          }
                        ],
                        "Next": "EnsureRetryCountForThrottling"
                      },
                      {
                        "And": [
                          {
                            "Variable": "$.status",
                            "StringEquals": "error"
                          },
                          {
                            "Variable": "$.error",
                            "StringMatches": "*Input is too large for processing*"
                          }
                        ],
                        "Next": "EnsureRetryCountForLargeInput"
                      },
                      {
                        "Variable": "$.status",
                        "StringEquals": "error",
                        "Next": "PrepareErrorFields"
                      }
                    ],
                    "Default": "WaitAfterProcessing"
                  },
                  "EnsureRetryCountForThrottling": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.retry_count",
                            "IsPresent": true
                          },
                          {
                            "Variable": "$.retry_count",
                            "NumericLessThan": 5
                          }
                        ],
                        "Next": "IncrementRetryAndWait"
                      }
                    ],
                    "Default": "InitializeRetryCountForThrottling"
                  },
                  "InitializeRetryCountForThrottling": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count": 0,
                      "max_retries": 5,
                      "status.$": "$.status",
                      "error.$": "$.error"
                    },
                    "Next": "IncrementRetryAndWait"
                  },
                  "EnsureRetryCountForLargeInput": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.retry_count",
                            "IsPresent": true
                          },
                          {
                            "Variable": "$.retry_count",
                            "NumericLessThan": 5
                          }
                        ],
                        "Next": "IncrementRetryAndWaitForLargeInput"
                      }
                    ],
                    "Default": "InitializeRetryCountForLargeInput"
                  },
                  "InitializeRetryCountForLargeInput": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count": 0,
                      "max_retries": 5,
                      "status.$": "$.status",
                      "error.$": "$.error"
                    },
                    "Next": "IncrementRetryAndWaitForLargeInput"
                  },
                  "WaitAfterProcessing": {
                    "Type": "Wait",
                    "Seconds": 120,
                    "Next": "ChunkSuccess"
                  },
                  "IncrementRetryAndWait": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "States.MathAdd($.retry_count, 1)",
                      "max_retries.$": "$.max_retries",
                      "wait_seconds": 180
                    },
                    "Next": "WaitBeforeRetry"
                  },
                  "IncrementRetryAndWaitForLargeInput": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "States.MathAdd($.retry_count, 1)",
                      "max_retries.$": "$.max_retries",
                      "wait_seconds": 60,
                      "error": "Input is too large for processing"
                    },
                    "Next": "WaitBeforeRetryLargeInput"
                  },
                  "WaitBeforeRetry": {
                    "Type": "Wait",
                    "Seconds": 180,
                    "Next": "ProcessChunk"
                  },
                  "WaitBeforeRetryLargeInput": {
                    "Type": "Wait",
                    "Seconds": 60,
                    "Next": "ProcessChunk"
                  },
                  "HandleChunkError": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "chunk_index.$": "$.chunk_index",
                      "error.$": "$.error",
                      "status": "error",
                      "message": "Chunk processing failed after retries"
                    },
                    "End": true
                  },
                  "ChunkSuccess": {
                    "Type": "Pass",
                    "End": true
                  }
                }
              },
              "MaxConcurrency": 1,
              "ResultPath": "$.chunk_results",
              "Next": "AggregateResults"
            },
            "AggregateResults": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${ResultAggregatorLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "chunk_results.$": "$.chunk_results"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "UpdateStatus"
            },
            "UpdateStatus": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${StatusLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path"
                }
              },
              "OutputPath": "$.Payload",
              "End": true
            },
            "UpdateStatusWithError": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${StatusLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "status": "ERROR",
                  "error.$": "$.error"
                }
              },
              "OutputPath": "$.Payload",
              "End": true
            }
          }
        }

Outputs:
  InitialLambdaArn:
    Description: ARN of the Initial Lambda function
    Value: !GetAtt InitialLambda.Arn
  
  ProcessFileLambdaArn:
    Description: ARN of the Process File Lambda function
    Value: !GetAtt ProcessFileLambda.Arn
  
  AggregateLambdaArn:
    Description: ARN of the Aggregate Lambda function
    Value: !GetAtt AggregateLambda.Arn
  
  AnalysisLambdaArn:
    Description: ARN of the Analysis Lambda function
    Value: !GetAtt AnalysisLambda.Arn
  
  StatusLambdaArn:
    Description: ARN of the Status Lambda function
    Value: !GetAtt StatusLambda.Arn
  
  StateMachineArn:
    Description: ARN of the Step Functions state machine
    Value: !Ref MainframeAnalyzerStateMachine
  
  JobsTableName:
    Description: Name of the DynamoDB table for job tracking
    Value: !Ref JobsTable
