AWSTemplateFormatVersion: '2010-09-09'
Description: 'Asynchronous Mainframe Documentation Analyzer'

Parameters:
  Environment:
    Type: String
    Default: dev
    AllowedValues:
      - dev
      - staging
      - prod
    Description: Environment name

  BedrockModelId:
    Type: String
    Default: 'arn:aws:bedrock:us-east-1:663362705389:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0'
    Description: Bedrock model ARN to use for analysis
  
  MaxFilesPerJob:
    Type: Number
    Default: 100
    Description: Maximum number of files to process per job
  
  ChunkingThreshold:
    Type: Number
    Default: 100000
    Description: Maximum combined characters for analysis

  MaxCombinedChars:
    Type: Number
    Default: 100000
    Description: Maximum combined characters for analysis

  MaxTokensPerChunk:
    Type: Number
    Default: 15000
    Description: Maximum tokens per chunk

  TargetLanguage:
    Type: String
    Default: python
    AllowedValues:
      - python
      - dotnet
      - java
      - go
      - javascript
    Description: Target programming language for generated artifacts

  LambdaCodeBucket:
    Type: String
    Description: S3 bucket containing Lambda code packages
    Default: ''
  
  ParameterStorePrefix:
    Type: String
    Description: Prefix for Parameter Store parameters
    Default: '/mainframe-modernization/documentation-agent/dev/updated-prompt'
  
  LambdaTimeout:
    Type: Number
    Default: 600
    Description: Timeout for Lambda functions in seconds
  
  LambdaMemory:
    Type: Number
    Default: 2048
    Description: Memory allocation for Lambda functions in MB

Conditions:
  CreateLambdaBucket: !Equals [!Ref LambdaCodeBucket, '']

Resources:
  # S3 Bucket for Prompt Storage
  PromptsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "mainframe-modernization-prompts-${Environment}-${AWS::AccountId}"
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 30
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Service
          Value: MainframeAnalyzer
        - Key: Purpose
          Value: PromptStorage

  # DynamoDB Table for Job Tracking
  JobsTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub MainframeAnalyzerJobs-${Environment}
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: job_id
          AttributeType: S
      KeySchema:
        - AttributeName: job_id
          KeyType: HASH
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

  # IAM Role for Lambda Functions
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: MainframeAnalyzerLambdaPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource: '*'
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:Query
                Resource: !GetAtt JobsTable.Arn
              - Effect: Allow
                Action:
                  - bedrock:InvokeModel
                  - bedrock:InvokeModelWithResponseStream
                Resource: '*'
              - Effect: Allow
                Action:
                  - states:StartExecution
                  - states:DescribeExecution
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                Resource: !Sub "${PromptsS3Bucket.Arn}/*"

  # IAM Role for Step Functions
  StepFunctionsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: MainframeAnalyzerStepFunctionsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: '*'

  # Lambda Functions
  InitialLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub MainframeAnalyzer-Initial-${Environment}
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/initial-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          STATE_MACHINE_ARN: !Ref MainframeAnalyzerStateMachine
          JOBS_TABLE_NAME: !Ref JobsTable
          MAX_FILES: !Ref MaxFilesPerJob

  PyPDFDocxLayer:
    Type: 'AWS::Lambda::LayerVersion'
    Properties:
      LayerName: pypdfdocx-layer
      Description: Layer containing PyPDF and DocX packages
      Content:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: layer/pypdfdocxlayer.zip
      CompatibleRuntimes:
        - python3.13
        - python3.12
        - python3.11

  ProcessFileLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ProcessFile
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/process-file-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Layers:
        - !Ref PyPDFDocxLayer
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable

  AggregateLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Aggregate
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/aggregate-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          MAX_COMBINED_CHARS: !Ref MaxCombinedChars

  ChunkingLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Chunking
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/chunking-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          CHUNKING_THRESHOLD: !Ref ChunkingThreshold
          MAX_TOKENS_PER_CHUNK: !Ref MaxTokensPerChunk

  ChunkProcessorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ChunkProcessor
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/chunk-processor-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          BEDROCK_MODEL_ID: !Ref BedrockModelId
          PROMPTS_BUCKET: !Ref PromptsS3Bucket
          TARGET_LANGUAGE: !Ref TargetLanguage
          CACHE_TTL_SECONDS: "300"

  ResultAggregatorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-ResultAggregator
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/result-aggregator-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          BEDROCK_MODEL_ID: !Ref BedrockModelId
          PROMPTS_BUCKET: !Ref PromptsS3Bucket
          TARGET_LANGUAGE: !Ref TargetLanguage
          CACHE_TTL_SECONDS: "300"

  AnalysisLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Analysis
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/analysis-lambda.zip
      Runtime: python3.13
      Timeout: !Ref LambdaTimeout
      MemorySize: !Ref LambdaMemory
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable
          BEDROCK_MODEL_ID: !Ref BedrockModelId
          PROMPTS_BUCKET: !Ref PromptsS3Bucket
          TARGET_LANGUAGE: !Ref TargetLanguage
          CACHE_TTL_SECONDS: "300"

  StatusLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: MainframeAnalyzer-Status
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !If 
          - CreateLambdaBucket
          - !Sub 'mainframe-analyzer-${Environment}-${AWS::AccountId}-${AWS::Region}'
          - !Ref LambdaCodeBucket
        S3Key: lambda/status-lambda.zip
      Runtime: python3.13
      Timeout: 30
      MemorySize: 256
      Environment:
        Variables:
          JOBS_TABLE_NAME: !Ref JobsTable

  # Step Functions State Machine
  MainframeAnalyzerStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub MainframeAnalyzerWorkflow-${Environment}
      RoleArn: !GetAtt StepFunctionsExecutionRole.Arn
      DefinitionString: !Sub |
        {
          "Comment": "Document Analysis Workflow with Chunking (No Initial Processing)",
          "StartAt": "ProcessFiles",
          "States": {
            "ProcessFiles": {
              "Type": "Map",
              "ItemsPath": "$.files",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "file_key.$": "$$.Map.Item.Value.key",
                "output_path.$": "$.output_path"
              },
              "Iterator": {
                "StartAt": "ProcessFile",
                "States": {
                  "ProcessFile": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "Parameters": {
                      "FunctionName": "${ProcessFileLambda.Arn}",
                      "Payload.$": "$"
                    },
                    "OutputPath": "$.Payload",
                    "End": true
                  }
                }
              },
              "ResultPath": "$.results",
              "Next": "AggregateFiles"
            },
            "AggregateFiles": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${AggregateLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "file_results.$": "$.results"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "EvaluateInputSize"
            },
            "EvaluateInputSize": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${ChunkingLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "full_prompt_key.$": "$.full_prompt_key",
                  "output_path.$": "$.output_path"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "NeedsChunking"
            },
            "NeedsChunking": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.requires_chunking",
                  "BooleanEquals": true,
                  "Next": "ProcessChunks"
                }
              ],
              "Default": "PrepareStandardAnalysis"
            },
            "PrepareStandardAnalysis": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count": 0,
                "max_retries": 5
              },
              "Next": "StandardAnalysis"
            },
            "StandardAnalysis": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${AnalysisLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "full_prompt_key.$": "$.full_prompt_key",
                  "output_path.$": "$.output_path"
                }
              },
              "Retry": [
                {
                  "ErrorEquals": ["States.TaskFailed"],
                  "IntervalSeconds": 180,
                  "MaxAttempts": 3,
                  "BackoffRate": 1.5
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": ["States.ALL"],
                  "ResultPath": "$.lambda_error",
                  "Next": "CheckStandardAnalysisError"
                }
              ],
              "OutputPath": "$.Payload",
              "Next": "CheckStandardAnalysisResult"
            },
            "CheckStandardAnalysisResult": {
              "Type": "Choice",
              "Choices": [
                {
                  "And": [
                    {
                      "Variable": "$.status",
                      "StringEquals": "error"
                    },
                    {
                      "Variable": "$.error",
                      "StringMatches": "*ThrottlingException*"
                    },
                    {
                      "Variable": "$.retry_count",
                      "NumericLessThan": 5
                    }
                  ],
                  "Next": "IncrementStandardRetryAndWait"
                },
                {
                  "Variable": "$.status",
                  "StringEquals": "error",
                  "Next": "UpdateStatusWithError"
                }
              ],
              "Default": "UpdateStatus"
            },
            "CheckStandardAnalysisError": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.lambda_error.errorMessage",
                  "StringMatches": "*Task timed out*",
                  "Next": "HandleStandardTimeout"
                }
              ],
              "Default": "UpdateStatusWithError"
            },
            "HandleStandardTimeout": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count.$": "$.retry_count",
                "max_retries.$": "$.max_retries",
                "error": "Lambda function timed out after 600 seconds"
              },
              "Next": "CheckStandardRetryCount"
            },
            "CheckStandardRetryCount": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.retry_count",
                  "NumericLessThan": 5,
                  "Next": "IncrementStandardRetryAndWait"
                }
              ],
              "Default": "UpdateStatusWithError"
            },
            "IncrementStandardRetryAndWait": {
              "Type": "Pass",
              "Parameters": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "full_prompt_key.$": "$.full_prompt_key",
                "output_path.$": "$.output_path",
                "retry_count.$": "States.MathAdd($.retry_count, 1)",
                "max_retries.$": "$.max_retries",
                "wait_seconds": 180
              },
              "Next": "WaitBeforeStandardRetry"
            },
            "WaitBeforeStandardRetry": {
              "Type": "Wait",
              "Seconds": 180,
              "Next": "StandardAnalysis"
            },
            "ProcessChunks": {
              "Type": "Map",
              "ItemsPath": "$.chunks",
              "ItemSelector": {
                "job_id.$": "$.job_id",
                "bucket_name.$": "$.bucket_name",
                "chunk_key.$": "$$.Map.Item.Value.chunk_key",
                "chunk_index.$": "$$.Map.Item.Value.index",
                "total_chunks.$": "$$.Map.Item.Value.total",
                "output_path.$": "$.output_path",
                "wait_seconds": 120,
                "retry_count": 0,
                "max_retries": 5
              },
              "Iterator": {
                "StartAt": "ProcessChunk",
                "States": {
                  "ProcessChunk": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::lambda:invoke",
                    "Parameters": {
                      "FunctionName": "${ChunkProcessorLambda.Arn}",
                      "Payload.$": "$"
                    },
                    "Retry": [
                      {
                        "ErrorEquals": ["States.TaskFailed"],
                        "IntervalSeconds": 180,
                        "MaxAttempts": 3,
                        "BackoffRate": 1.5
                      }
                    ],
                    "Catch": [
                      {
                        "ErrorEquals": ["States.ALL"],
                        "ResultPath": "$.lambda_error",
                        "Next": "CheckLambdaError"
                      }
                    ],
                    "OutputPath": "$.Payload",
                    "Next": "CheckForThrottling"
                  },
                  "CheckLambdaError": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "Variable": "$.lambda_error.errorMessage",
                        "StringMatches": "*Task timed out*",
                        "Next": "HandleLambdaTimeout"
                      }
                    ],
                    "Default": "PrepareErrorFields"
                  },
                  "PrepareErrorFields": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "chunk_index.$": "$.chunk_index",
                      "error.$": "States.JsonToString($.lambda_error)",
                      "status": "error",
                      "message": "Chunk processing failed after retries"
                    },
                    "Next": "HandleChunkError"
                  },
                  "HandleLambdaTimeout": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "$.retry_count",
                      "max_retries.$": "$.max_retries",
                      "error": "Lambda function timed out after 600 seconds"
                    },
                    "Next": "CheckRetryCount"
                  },
                  "CheckRetryCount": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "Variable": "$.retry_count",
                        "NumericLessThan": 5,
                        "Next": "IncrementRetryAndWait"
                      }
                    ],
                    "Default": "PrepareErrorFields"
                  },
                  "CheckForThrottling": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.status",
                            "StringEquals": "error"
                          },
                          {
                            "Variable": "$.error",
                            "StringMatches": "*ThrottlingException*"
                          }
                        ],
                        "Next": "EnsureRetryCountForThrottling"
                      },
                      {
                        "And": [
                          {
                            "Variable": "$.status",
                            "StringEquals": "error"
                          },
                          {
                            "Variable": "$.error",
                            "StringMatches": "*Input is too large for processing*"
                          }
                        ],
                        "Next": "EnsureRetryCountForLargeInput"
                      },
                      {
                        "Variable": "$.status",
                        "StringEquals": "error",
                        "Next": "PrepareErrorFields"
                      }
                    ],
                    "Default": "WaitAfterProcessing"
                  },
                  "EnsureRetryCountForThrottling": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.retry_count",
                            "IsPresent": true
                          },
                          {
                            "Variable": "$.retry_count",
                            "NumericLessThan": 5
                          }
                        ],
                        "Next": "IncrementRetryAndWait"
                      }
                    ],
                    "Default": "InitializeRetryCountForThrottling"
                  },
                  "InitializeRetryCountForThrottling": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count": 0,
                      "max_retries": 5,
                      "status.$": "$.status",
                      "error.$": "$.error"
                    },
                    "Next": "IncrementRetryAndWait"
                  },
                  "EnsureRetryCountForLargeInput": {
                    "Type": "Choice",
                    "Choices": [
                      {
                        "And": [
                          {
                            "Variable": "$.retry_count",
                            "IsPresent": true
                          },
                          {
                            "Variable": "$.retry_count",
                            "NumericLessThan": 5
                          }
                        ],
                        "Next": "IncrementRetryAndWaitForLargeInput"
                      }
                    ],
                    "Default": "InitializeRetryCountForLargeInput"
                  },
                  "InitializeRetryCountForLargeInput": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count": 0,
                      "max_retries": 5,
                      "status.$": "$.status",
                      "error.$": "$.error"
                    },
                    "Next": "IncrementRetryAndWaitForLargeInput"
                  },
                  "WaitAfterProcessing": {
                    "Type": "Wait",
                    "Seconds": 120,
                    "Next": "ChunkSuccess"
                  },
                  "IncrementRetryAndWait": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "States.MathAdd($.retry_count, 1)",
                      "max_retries.$": "$.max_retries",
                      "wait_seconds": 180
                    },
                    "Next": "WaitBeforeRetry"
                  },
                  "IncrementRetryAndWaitForLargeInput": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "bucket_name.$": "$.bucket_name",
                      "chunk_key.$": "$.chunk_key",
                      "chunk_index.$": "$.chunk_index",
                      "total_chunks.$": "$.total_chunks",
                      "output_path.$": "$.output_path",
                      "retry_count.$": "States.MathAdd($.retry_count, 1)",
                      "max_retries.$": "$.max_retries",
                      "wait_seconds": 60,
                      "error": "Input is too large for processing"
                    },
                    "Next": "WaitBeforeRetryLargeInput"
                  },
                  "WaitBeforeRetry": {
                    "Type": "Wait",
                    "Seconds": 180,
                    "Next": "ProcessChunk"
                  },
                  "WaitBeforeRetryLargeInput": {
                    "Type": "Wait",
                    "Seconds": 60,
                    "Next": "ProcessChunk"
                  },
                  "HandleChunkError": {
                    "Type": "Pass",
                    "Parameters": {
                      "job_id.$": "$.job_id",
                      "chunk_index.$": "$.chunk_index",
                      "error.$": "$.error",
                      "status": "error",
                      "message": "Chunk processing failed after retries"
                    },
                    "End": true
                  },
                  "ChunkSuccess": {
                    "Type": "Pass",
                    "End": true
                  }
                }
              },
              "MaxConcurrency": 1,
              "ResultPath": "$.chunk_results",
              "Next": "AggregateResults"
            },
            "AggregateResults": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${ResultAggregatorLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "chunk_results.$": "$.chunk_results"
                }
              },
              "OutputPath": "$.Payload",
              "Next": "UpdateStatus"
            },
            "UpdateStatus": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${StatusLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path"
                }
              },
              "OutputPath": "$.Payload",
              "End": true
            },
            "UpdateStatusWithError": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${StatusLambda.Arn}",
                "Payload": {
                  "job_id.$": "$.job_id",
                  "bucket_name.$": "$.bucket_name",
                  "output_path.$": "$.output_path",
                  "status": "ERROR",
                  "error.$": "$.error"
                }
              },
              "OutputPath": "$.Payload",
              "End": true
            }
          }
        }

Outputs:
  InitialLambdaArn:
    Description: ARN of the Initial Lambda function
    Value: !GetAtt InitialLambda.Arn
  
  ProcessFileLambdaArn:
    Description: ARN of the Process File Lambda function
    Value: !GetAtt ProcessFileLambda.Arn
  
  AggregateLambdaArn:
    Description: ARN of the Aggregate Lambda function
    Value: !GetAtt AggregateLambda.Arn
  
  AnalysisLambdaArn:
    Description: ARN of the Analysis Lambda function
    Value: !GetAtt AnalysisLambda.Arn
  
  StatusLambdaArn:
    Description: ARN of the Status Lambda function
    Value: !GetAtt StatusLambda.Arn
  
  StateMachineArn:
    Description: ARN of the Step Functions state machine
    Value: !Ref MainframeAnalyzerStateMachine
  
  JobsTableName:
    Description: Name of the DynamoDB table for job tracking
    Value: !Ref JobsTable
  
  PromptsS3Bucket:
    Description: Name of the S3 bucket for storing prompts
    Value: !Ref PromptsS3Bucket
